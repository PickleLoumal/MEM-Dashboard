#!/usr/bin/env python3
"""
Banking SectoræŒ‡æ ‡æ•°æ®æŠ“å–å™¨
åŸºäºå®é™…ä½¿ç”¨çš„fetch_money_supply_indicators.pyæ¶æ„
ä¸¥æ ¼éµå¾ªä¼ä¸šçº§æ•°æ®å¤„ç†æ ‡å‡†

Banking SectoræŒ‡æ ‡(8ä¸ªæŒ‡æ ‡):
1. FEDFUNDS - Federal Funds Rate (Effective)
2. IORB - Interest on Reserve Balances
3. TOTRESNS - Total Reserve Balances  
4. WALCL - Federal Reserve Balance Sheet (Total Assets)
5. PCEPI - PCE Price Index (Inflation)
6. UNRATE - Unemployment Rate
7. TOTLL - Commercial Bank Loans and Leases
8. DPRIME - Bank Prime Loan Rate
"""

import os
import sys
import django
from datetime import datetime, timedelta
import time
import logging
import json
from typing import Dict, List, Optional, Tuple

# æ·»åŠ Djangoé¡¹ç›®è·¯å¾„
sys.path.append('/Volumes/Pickle Samsung SSD/MEM Dashboard 2/src/django_api')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'django_api.settings')

# åˆå§‹åŒ–Django
django.setup()

from fred_us.models import FredUsIndicator
from fred_us.data_fetcher import UsFredDataFetcher
from django.db import transaction, IntegrityError
from django.core.exceptions import ValidationError

# é…ç½®ä¼ä¸šçº§æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'/Volumes/Pickle Samsung SSD/MEM Dashboard 2/logs/fetch_banking_sector_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BankingSectorDataFetcher:
    """
    Banking Sectoræ•°æ®æŠ“å–å™¨
    å®ç°ä¼ä¸šçº§æ•°æ®å¤„ç†æ ‡å‡†ï¼š
    - äº‹åŠ¡å®‰å…¨æ€§
    - é”™è¯¯æ¢å¤æœºåˆ¶  
    - æ€§èƒ½ç›‘æ§
    - æ•°æ®è´¨é‡éªŒè¯
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®æŠ“å–å™¨"""
        logger.info("ğŸš€ åˆå§‹åŒ–Banking Sectoræ•°æ®æŠ“å–å™¨...")
        
        self.fetcher = UsFredDataFetcher()
        
        # Banking SectoræŒ‡æ ‡é…ç½® - 8ä¸ªæŒ‡æ ‡
        self.indicators = {
            'FEDFUNDS': {
                'name': 'Federal Funds Rate (Effective)',
                'type': 'Banking Sector',
                'description': 'è”é‚¦åŸºé‡‘åˆ©ç‡ - ç¾è”å‚¨è´§å¸æ”¿ç­–åŸºå‡†åˆ©ç‡',
                'unit': 'Percent',
                'validation': {
                    'min_value': 0,
                    'max_value': 25,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'interest_rate'
                }
            },
            'IORB': {
                'name': 'Interest Rate on Reserve Balances',
                'type': 'Banking Sector', 
                'description': 'å‡†å¤‡é‡‘ä½™é¢åˆ©ç‡ - é“¶è¡Œåœ¨ç¾è”å‚¨å­˜æ¬¾çš„åˆ©ç‡',
                'unit': 'Percent',
                'validation': {
                    'min_value': 0,
                    'max_value': 25,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'interest_rate'
                }
            },
            'TOTRESNS': {
                'name': 'Total Reserve Balances',
                'type': 'Banking Sector',
                'description': 'æ€»å‡†å¤‡é‡‘ä½™é¢ - å­˜æ¬¾æœºæ„åœ¨ç¾è”å‚¨çš„æ€»å‡†å¤‡é‡‘',
                'unit': 'Billions of Dollars',
                'validation': {
                    'min_value': 0,
                    'max_value': None,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'balance_sheet'
                }
            },
            'WALCL': {
                'name': 'Federal Reserve Balance Sheet Total Assets',
                'type': 'Banking Sector',
                'description': 'ç¾è”å‚¨èµ„äº§è´Ÿå€ºè¡¨æ€»èµ„äº§ - ç¾è”å‚¨æŒæœ‰çš„æ€»èµ„äº§è§„æ¨¡',
                'unit': 'Millions of Dollars',
                'validation': {
                    'min_value': 0,
                    'max_value': None,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'balance_sheet'
                }
            },
            'PCEPI': {
                'name': 'PCE Price Index',
                'type': 'Banking Sector',
                'description': 'PCEä»·æ ¼æŒ‡æ•° - ç¾è”å‚¨åå¥½çš„é€šèƒ€æŒ‡æ ‡',
                'unit': 'Index 2017=100',
                'validation': {
                    'min_value': 50,
                    'max_value': 200,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'price_index'
                }
            },
            'UNRATE': {
                'name': 'Unemployment Rate',
                'type': 'Banking Sector',
                'description': 'å¤±ä¸šç‡ - åŠ³åŠ¨åŠ›å¸‚åœºå¥åº·åº¦çš„å…³é”®æŒ‡æ ‡',
                'unit': 'Percent',
                'validation': {
                    'min_value': 0,
                    'max_value': 30,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'unemployment'
                }
            },
            'TOTLL': {
                'name': 'Commercial Bank Loans and Leases',
                'type': 'Banking Sector',
                'description': 'å•†ä¸šé“¶è¡Œè´·æ¬¾å’Œç§Ÿèµ - é“¶è¡Œä¿¡è´·æ´»åŠ¨çš„æ ¸å¿ƒæŒ‡æ ‡',
                'unit': 'Billions of Dollars',
                'validation': {
                    'min_value': 0,
                    'max_value': None,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'balance_sheet'
                }
            },
            'DPRIME': {
                'name': 'Bank Prime Loan Rate',
                'type': 'Banking Sector',
                'description': 'é“¶è¡ŒåŸºå‡†è´·æ¬¾åˆ©ç‡ - é“¶è¡Œå‘æœ€ä¼˜è´¨å®¢æˆ·æä¾›çš„åˆ©ç‡',
                'unit': 'Percent',
                'validation': {
                    'min_value': 0,
                    'max_value': 30,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'prime_rate'
                }
            }
        }
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_fetched': 0,
            'total_saved': 0,
            'total_errors': 0,
            'series_processed': 0,
            'start_time': datetime.now(),
            'processing_times': {},
            'error_details': []
        }
        
        logger.info(f"ğŸ“Š å·²é…ç½® {len(self.indicators)} ä¸ªBanking SectoræŒ‡æ ‡")
    
    def validate_observation(self, obs: dict, series_id: str) -> Tuple[bool, str]:
        """
        ä¼ä¸šçº§æ•°æ®éªŒè¯
        åŒ…å«å¤šå±‚éªŒè¯ï¼šå­—æ®µéªŒè¯ã€æ•°å€¼éªŒè¯ã€ä¸šåŠ¡è§„åˆ™éªŒè¯
        """
        try:
            validation_config = self.indicators[series_id]['validation']
            
            # ç¬¬ä¸€å±‚ï¼šå¿…éœ€å­—æ®µéªŒè¯
            for field in validation_config['required_fields']:
                if field not in obs or obs[field] is None:
                    return False, f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
            
            # ç¬¬äºŒå±‚ï¼šæ•°å€¼æœ‰æ•ˆæ€§éªŒè¯
            if obs['value'] == '.' or obs['value'] == '' or obs['value'] is None:
                return False, "ç©ºå€¼æˆ–æ— æ•ˆå€¼"
            
            try:
                value = float(obs['value'])
            except (ValueError, TypeError):
                return False, f"æ— æ³•è½¬æ¢ä¸ºæ•°å€¼: {obs['value']}"
            
            # ç¬¬ä¸‰å±‚ï¼šæ•°å€¼èŒƒå›´éªŒè¯
            if validation_config['min_value'] is not None and value < validation_config['min_value']:
                return False, f"å€¼ä½äºæœ€å°å€¼: {value} < {validation_config['min_value']}"
            
            if validation_config['max_value'] is not None and value > validation_config['max_value']:
                return False, f"å€¼è¶…è¿‡æœ€å¤§å€¼: {value} > {validation_config['max_value']}"
            
            # ç¬¬å››å±‚ï¼šæ—¥æœŸæ ¼å¼éªŒè¯
            try:
                date_obj = datetime.strptime(obs['date'], '%Y-%m-%d').date()
                if date_obj > datetime.now().date():
                    return False, f"æœªæ¥æ—¥æœŸ: {obs['date']}"
            except ValueError:
                return False, f"æ— æ•ˆæ—¥æœŸæ ¼å¼: {obs['date']}"
            
            # ç¬¬äº”å±‚ï¼šä¸šåŠ¡è§„åˆ™éªŒè¯
            business_validation = self.validate_business_rules(value, series_id, validation_config['business_rules'])
            if not business_validation[0]:
                return False, f"ä¸šåŠ¡è§„åˆ™è¿å: {business_validation[1]}"
            
            return True, "éªŒè¯é€šè¿‡"
            
        except Exception as e:
            return False, f"éªŒè¯å¼‚å¸¸: {str(e)}"
    
    def validate_business_rules(self, value: float, series_id: str, rule_type: str) -> Tuple[bool, str]:
        """ä¸šåŠ¡è§„åˆ™éªŒè¯"""
        try:
            if rule_type == 'interest_rate':
                # åˆ©ç‡ä¸šåŠ¡è§„åˆ™
                if value < 0:
                    return False, f"è´Ÿåˆ©ç‡éœ€è¦ç‰¹æ®Šå…³æ³¨: {value}%"
                if value > 20:
                    return False, f"å¼‚å¸¸é«˜åˆ©ç‡: {value}%"
                    
            elif rule_type == 'balance_sheet':
                # èµ„äº§è´Ÿå€ºè¡¨è§„åˆ™
                if value <= 0:
                    return False, f"èµ„äº§å€¼ä¸èƒ½ä¸ºè´Ÿæˆ–é›¶: {value}"
                    
            elif rule_type == 'price_index':
                # ä»·æ ¼æŒ‡æ•°è§„åˆ™
                if value <= 0:
                    return False, f"ä»·æ ¼æŒ‡æ•°å¿…é¡»ä¸ºæ­£: {value}"
                    
            elif rule_type == 'unemployment':
                # å¤±ä¸šç‡è§„åˆ™
                if value < 0 or value > 100:
                    return False, f"å¤±ä¸šç‡è¶…å‡ºåˆç†èŒƒå›´: {value}%"
                    
            elif rule_type == 'prime_rate':
                # é“¶è¡ŒåŸºå‡†åˆ©ç‡è§„åˆ™
                if value < 0:
                    return False, f"åŸºå‡†åˆ©ç‡ä¸èƒ½ä¸ºè´Ÿ: {value}%"
                if value > 25:
                    return False, f"åŸºå‡†åˆ©ç‡å¼‚å¸¸é«˜: {value}%"
            
            return True, "ä¸šåŠ¡è§„åˆ™éªŒè¯é€šè¿‡"
            
        except Exception as e:
            return False, f"ä¸šåŠ¡è§„åˆ™éªŒè¯å¼‚å¸¸: {str(e)}"
    
    def save_observations_batch(self, observations: List[dict], series_id: str) -> Tuple[int, int]:
        """
        æ‰¹é‡ä¿å­˜è§‚æµ‹æ•°æ®
        å®ç°äº‹åŠ¡å®‰å…¨ã€é”™è¯¯æ¢å¤ã€æ€§èƒ½ä¼˜åŒ–
        """
        saved_count = 0
        error_count = 0
        batch_size = 100
        
        indicator_info = self.indicators[series_id]
        logger.info(f"ğŸ’¾ å¼€å§‹æ‰¹é‡ä¿å­˜ {series_id}: {len(observations)} æ¡æ•°æ®")
        
        # åˆ†æ‰¹å¤„ç†ä»¥ä¼˜åŒ–æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨
        for i in range(0, len(observations), batch_size):
            batch = observations[i:i + batch_size]
            batch_start_time = time.time()
            
            try:
                with transaction.atomic():
                    for obs in batch:
                        # æ•°æ®éªŒè¯
                        is_valid, message = self.validate_observation(obs, series_id)
                        if not is_valid:
                            logger.debug(f"âš ï¸ {series_id} è·³è¿‡æ— æ•ˆæ•°æ®: {message}")
                            error_count += 1
                            continue
                        
                        try:
                            value = float(obs['value'])
                            date = datetime.strptime(obs['date'], '%Y-%m-%d').date()
                            
                            # ä½¿ç”¨get_or_createé¿å…é‡å¤æ•°æ®
                            indicator, created = FredUsIndicator.objects.get_or_create(
                                series_id=series_id,
                                date=date,
                                defaults={
                                    'indicator_name': indicator_info['name'],
                                    'indicator_type': indicator_info['type'],
                                    'value': value,
                                    'source': 'FRED',
                                    'unit': indicator_info.get('unit', ''),
                                    'frequency': '',
                                    'metadata': {
                                        'description': indicator_info['description'],
                                        'original_value': obs['value'],
                                        'fetched_at': datetime.now().isoformat(),
                                        'validation_passed': True
                                    }
                                }
                            )
                            
                            if created:
                                saved_count += 1
                            
                        except IntegrityError as e:
                            logger.debug(f"âš ï¸ {series_id} æ•°æ®é‡å¤: {str(e)}")
                            continue
                        except ValidationError as e:
                            logger.warning(f"âš ï¸ {series_id} æ•°æ®éªŒè¯å¤±è´¥: {str(e)}")
                            error_count += 1
                            continue
                        except Exception as e:
                            logger.error(f"âŒ {series_id} ä¿å­˜å¼‚å¸¸: {str(e)}")
                            error_count += 1
                            continue
                
                batch_time = time.time() - batch_start_time
                logger.debug(f"ğŸ“¦ {series_id} æ‰¹æ¬¡ {i//batch_size + 1} å®Œæˆ: {batch_time:.2f}ç§’")
                
            except Exception as e:
                logger.error(f"âŒ {series_id} æ‰¹æ¬¡äº‹åŠ¡å¤±è´¥: {str(e)}")
                error_count += len(batch)
                self.stats['error_details'].append({
                    'series_id': series_id,
                    'batch': i//batch_size + 1,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
                continue
        
        logger.info(f"âœ… {series_id} æ‰¹é‡ä¿å­˜å®Œæˆ: {saved_count} æ–°å¢, {error_count} é”™è¯¯")
        return saved_count, error_count
    
    def fetch_series_with_retry(self, series_id: str, max_retries: int = 3, delay: float = 1.0) -> Optional[List[dict]]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„æ•°æ®è·å–
        å®ç°æŒ‡æ•°é€€é¿ç®—æ³•å’Œè¯¦ç»†é”™è¯¯è®°å½•
        """
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ”„ å°è¯•è·å– {series_id} (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)")
                
                # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
                request_start = time.time()
                
                # è·å–æ›´å¤šå†å²æ•°æ®ä»¥ç¡®ä¿å®Œæ•´æ€§
                observations = self.fetcher.get_series_observations(series_id, limit=1000)
                
                request_time = time.time() - request_start
                self.stats['processing_times'][f"{series_id}_request_{attempt+1}"] = request_time
                
                if observations:
                    logger.info(f"âœ… æˆåŠŸè·å– {series_id}: {len(observations)} æ¡æ•°æ® ({request_time:.2f}ç§’)")
                    return observations
                else:
                    logger.warning(f"âš ï¸ {series_id}: APIè¿”å›ç©ºæ•°æ®")
                    
            except Exception as e:
                error_msg = f"{series_id} è·å–å¤±è´¥ (å°è¯• {attempt + 1}): {str(e)}"
                logger.error(f"âŒ {error_msg}")
                
                self.stats['error_details'].append({
                    'series_id': series_id,
                    'attempt': attempt + 1,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
                
                if attempt < max_retries - 1:
                    wait_time = delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    logger.info(f"â³ ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
        
        logger.error(f"ğŸ’¥ {series_id} æ‰€æœ‰é‡è¯•å‡å¤±è´¥")
        return None
    
    def fetch_indicator(self, series_id: str) -> bool:
        """è·å–å•ä¸ªæŒ‡æ ‡çš„å®Œæ•´æ•°æ®å¤„ç†æµç¨‹"""
        start_time = time.time()
        logger.info(f"\n{'='*50} å¤„ç†æŒ‡æ ‡: {series_id} {'='*50}")
        
        try:
            # æ£€æŸ¥ç°æœ‰æ•°æ®
            existing_count = FredUsIndicator.objects.filter(series_id=series_id).count()
            logger.info(f"ğŸ“Š æ•°æ®åº“ç°æœ‰è®°å½•: {existing_count}")
            
            # è·å–æ•°æ®
            observations = self.fetch_series_with_retry(series_id)
            if not observations:
                logger.error(f"âŒ {series_id}: æ— æ³•è·å–æ•°æ®")
                self.stats['total_errors'] += 1
                return False
            
            self.stats['total_fetched'] += len(observations)
            
            # æ•°æ®è´¨é‡é¢„æ£€æŸ¥
            valid_data_count = sum(1 for obs in observations 
                                 if obs['value'] not in ['.', '', None])
            quality_ratio = valid_data_count / len(observations) if observations else 0
            
            logger.info(f"ğŸ“ˆ æ•°æ®è´¨é‡é¢„æ£€: {valid_data_count}/{len(observations)} æœ‰æ•ˆæ•°æ® ({quality_ratio*100:.1f}%)")
            
            if quality_ratio < 0.8:
                logger.warning(f"âš ï¸ {series_id} æ•°æ®è´¨é‡è¾ƒä½: {quality_ratio*100:.1f}%")
            
            # ä¿å­˜æ•°æ®
            logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜ {len(observations)} æ¡è§‚æµ‹æ•°æ®...")
            saved_count, error_count = self.save_observations_batch(observations, series_id)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_saved'] += saved_count
            self.stats['total_errors'] += error_count
            self.stats['series_processed'] += 1
            
            # éªŒè¯ä¿å­˜ç»“æœ
            final_count = FredUsIndicator.objects.filter(series_id=series_id).count()
            latest_record = FredUsIndicator.objects.filter(series_id=series_id).order_by('-date').first()
            
            processing_time = time.time() - start_time
            self.stats['processing_times'][f"{series_id}_total"] = processing_time
            
            # è¯¦ç»†ç»“æœæŠ¥å‘Š
            logger.info(f"âœ… {series_id} å¤„ç†å®Œæˆ:")
            logger.info(f"   ğŸ“¥ ä»APIè·å–: {len(observations)} æ¡")
            logger.info(f"   ğŸ’¾ æ–°ä¿å­˜: {saved_count} æ¡")
            logger.info(f"   âŒ é”™è¯¯: {error_count} æ¡") 
            logger.info(f"   ğŸ“Š æ•°æ®åº“æ€»è®¡: {final_count} æ¡")
            logger.info(f"   â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            
            if latest_record:
                logger.info(f"   ğŸ“… æœ€æ–°æ•°æ®: {latest_record.value} ({latest_record.date})")
            
            return final_count > 0
            
        except Exception as e:
            logger.error(f"ğŸ’¥ {series_id} å¤„ç†å¼‚å¸¸: {str(e)}")
            self.stats['total_errors'] += 1
            self.stats['error_details'].append({
                'series_id': series_id,
                'error': str(e),
                'phase': 'fetch_indicator',
                'timestamp': datetime.now().isoformat()
            })
            return False
    
    def generate_comprehensive_report(self) -> dict:
        """ç”Ÿæˆè¯¦ç»†çš„æ‰§è¡ŒæŠ¥å‘Š"""
        duration = datetime.now() - self.stats['start_time']
        
        report = {
            'execution_summary': {
                'start_time': self.stats['start_time'].isoformat(),
                'end_time': datetime.now().isoformat(),
                'total_duration': str(duration),
                'duration_seconds': duration.total_seconds()
            },
            'processing_statistics': {
                'indicators_configured': len(self.indicators),
                'series_processed': self.stats['series_processed'],
                'total_fetched': self.stats['total_fetched'],
                'total_saved': self.stats['total_saved'],
                'total_errors': self.stats['total_errors'],
                'success_rate': (self.stats['total_saved'] / self.stats['total_fetched'] * 100) if self.stats['total_fetched'] > 0 else 0
            },
            'performance_metrics': {
                'processing_times': self.stats['processing_times'],
                'average_processing_time': sum(self.stats['processing_times'].values()) / len(self.stats['processing_times']) if self.stats['processing_times'] else 0,
                'throughput_records_per_second': self.stats['total_fetched'] / duration.total_seconds() if duration.total_seconds() > 0 else 0
            },
            'data_quality': {},
            'error_analysis': {
                'error_count': len(self.stats['error_details']),
                'error_details': self.stats['error_details']
            }
        }
        
        # ç”Ÿæˆæ¯ä¸ªæŒ‡æ ‡çš„è¯¦ç»†çŠ¶æ€
        for series_id in self.indicators.keys():
            count = FredUsIndicator.objects.filter(series_id=series_id).count()
            if count > 0:
                latest = FredUsIndicator.objects.filter(series_id=series_id).order_by('-date').first()
                oldest = FredUsIndicator.objects.filter(series_id=series_id).order_by('date').first()
                
                report['data_quality'][series_id] = {
                    'total_records': count,
                    'latest_date': latest.date.isoformat() if latest else None,
                    'latest_value': latest.value if latest else None,
                    'oldest_date': oldest.date.isoformat() if oldest else None,
                    'data_span_days': (latest.date - oldest.date).days if latest and oldest else 0,
                    'status': 'success'
                }
            else:
                report['data_quality'][series_id] = {
                    'total_records': 0,
                    'status': 'failed'
                }
        
        return report
    
    def log_comprehensive_report(self):
        """è®°å½•è¯¦ç»†çš„æ‰§è¡ŒæŠ¥å‘Š"""
        report = self.generate_comprehensive_report()
        
        logger.info("\n" + "="*100)
        logger.info("ğŸ“Š Banking Sectoræ•°æ®æŠ“å– - ç»¼åˆæ‰§è¡ŒæŠ¥å‘Š")
        logger.info("="*100)
        
        # æ‰§è¡Œæ‘˜è¦
        logger.info(f"â° æ‰§è¡Œæ—¶é—´: {report['execution_summary']['total_duration']}")
        logger.info(f"ğŸ“ˆ å¤„ç†åºåˆ—: {report['processing_statistics']['series_processed']}/{len(self.indicators)}")
        logger.info(f"ğŸ“¥ æ€»è·å–é‡: {report['processing_statistics']['total_fetched']} æ¡")
        logger.info(f"ğŸ’¾ æ€»ä¿å­˜é‡: {report['processing_statistics']['total_saved']} æ¡")
        logger.info(f"âŒ æ€»é”™è¯¯æ•°: {report['processing_statistics']['total_errors']} æ¡")
        logger.info(f"âœ… æˆåŠŸç‡: {report['processing_statistics']['success_rate']:.1f}%")
        
        # æ€§èƒ½æŒ‡æ ‡
        logger.info(f"âš¡ å¹³å‡å¤„ç†æ—¶é—´: {report['performance_metrics']['average_processing_time']:.2f} ç§’")
        logger.info(f"ğŸš€ å¤„ç†ååé‡: {report['performance_metrics']['throughput_records_per_second']:.1f} æ¡/ç§’")
        
        # å„æŒ‡æ ‡è¯¦ç»†çŠ¶æ€
        logger.info("\nğŸ“‹ å„æŒ‡æ ‡è¯¦ç»†çŠ¶æ€:")
        for series_id, info in self.indicators.items():
            quality_info = report['data_quality'].get(series_id, {})
            if quality_info.get('status') == 'success':
                logger.info(f"âœ… {series_id}: {quality_info['total_records']} æ¡è®°å½•")
                logger.info(f"   ğŸ“… æ—¶é—´èŒƒå›´: {quality_info['oldest_date']} åˆ° {quality_info['latest_date']}")
                logger.info(f"   ğŸ“ˆ æœ€æ–°å€¼: {quality_info['latest_value']}")
            else:
                logger.info(f"âŒ {series_id}: æŠ“å–å¤±è´¥")
        
        # é”™è¯¯åˆ†æ
        if report['error_analysis']['error_count'] > 0:
            logger.warning(f"\nâš ï¸ é”™è¯¯åˆ†æ: å…±å‘ç° {report['error_analysis']['error_count']} ä¸ªé”™è¯¯")
            for error in report['error_analysis']['error_details'][-5:]:  # æ˜¾ç¤ºæœ€è¿‘5ä¸ªé”™è¯¯
                logger.warning(f"   - {error.get('series_id', 'Unknown')}: {error.get('error', 'Unknown error')}")
    
    def run(self) -> bool:
        """ä¸»æ‰§è¡Œæµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹Banking SectoræŒ‡æ ‡æ•°æ®æŠ“å–...")
        logger.info(f"ğŸ“Š ç›®æ ‡æŒ‡æ ‡: {list(self.indicators.keys())}")
        logger.info(f"ğŸ¯ é¢„æœŸæŠ“å–: {len(self.indicators)} ä¸ªç³»åˆ—")
        
        success_count = 0
        
        try:
            for series_id in self.indicators.keys():
                try:
                    if self.fetch_indicator(series_id):
                        success_count += 1
                    
                    # APIè¯·æ±‚é—´éš”ï¼Œé¿å…é€Ÿç‡é™åˆ¶
                    time.sleep(0.5)
                    
                except KeyboardInterrupt:
                    logger.warning("âš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
                    break
                except Exception as e:
                    logger.error(f"âŒ {series_id} å¤„ç†å¼‚å¸¸: {str(e)}")
                    continue
            
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            self.log_comprehensive_report()
            
            # æ‰§è¡Œç»“æœåˆ¤æ–­
            if success_count == len(self.indicators):
                logger.info("ğŸ‰ æ‰€æœ‰Banking SectoræŒ‡æ ‡æ•°æ®æŠ“å–å®Œæˆ")
                return True
            else:
                logger.warning(f"âš ï¸ éƒ¨åˆ†æŒ‡æ ‡å¤„ç†å¤±è´¥: {success_count}/{len(self.indicators)}")
                return success_count > 0  # åªè¦æœ‰ä¸€ä¸ªæˆåŠŸå°±ç®—éƒ¨åˆ†æˆåŠŸ
                
        except Exception as e:
            logger.error(f"ğŸ’¥ ç³»ç»Ÿçº§å¼‚å¸¸: {str(e)}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("="*100)
    logger.info("Banking Sector FREDæ•°æ®æŠ“å–å™¨ v2.0")
    logger.info("ä¼ä¸šçº§æ•°æ®å¤„ç† | äº‹åŠ¡å®‰å…¨ | æ€§èƒ½ç›‘æ§")
    logger.info("="*100)
    
    try:
        fetcher = BankingSectorDataFetcher()
        success = fetcher.run()
        
        if success:
            logger.info("âœ… Banking Sectoræ•°æ®æŠ“å–ä»»åŠ¡å®Œæˆ")
            sys.exit(0)
        else:
            logger.error("âŒ Banking Sectoræ•°æ®æŠ“å–ä»»åŠ¡å¤±è´¥")
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"ğŸ’¥ ç³»ç»Ÿå¼‚å¸¸: {str(e)}")
        sys.exit(2)

if __name__ == "__main__":
    main()
