#!/usr/bin/env python3
"""
InflationæŒ‡æ ‡æ•°æ®æŠ“å–å™¨
åŸºäºå®é™…ä½¿ç”¨çš„fetch_banking_sector_indicators.pyæ¶æ„
ä¸¥æ ¼éµå¾ªä¼ä¸šçº§æ•°æ®å¤„ç†æ ‡å‡†

InflationæŒ‡æ ‡(8ä¸ªæŒ‡æ ‡):
1. CPIAUCSL - Consumer Price Index (CPI)
2. PCEPILFE - Core PCE Price Index
3. FEDFUNDS - Federal Funds Rate
4. UNRATE - Unemployment Rate
5. RSAFS - Retail Sales
6. PPIACO - Producer Price Index: All Commodities
7. T10YIEM - 10-Year Breakeven Inflation Rate
8. DCOILWTICO - Crude Oil Prices (WTI)
"""

import os
import sys
import django
from datetime import datetime, timedelta
import time
import logging
import json
from typing import Dict, List, Optional, Tuple

# æ·»åŠ Djangoé¡¹ç›®è·¯å¾„
sys.path.append('/Volumes/Pickle Samsung SSD/MEM Dashboard 2/src/django_api')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'django_api.settings')

# åˆå§‹åŒ–Django
django.setup()

from fred_us.models import FredUsIndicator
from fred_us.data_fetcher import UsFredDataFetcher
from django.db import transaction, IntegrityError
from django.core.exceptions import ValidationError

# é…ç½®ä¼ä¸šçº§æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'/Volumes/Pickle Samsung SSD/MEM Dashboard 2/logs/fetch_inflation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class InflationDataFetcher:
    """Inflationæ•°æ®æŠ“å–å™¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®æŠ“å–å™¨"""
        logger.info("ğŸš€ åˆå§‹åŒ–Inflationæ•°æ®æŠ“å–å™¨...")
        
        self.fetcher = UsFredDataFetcher()
        
        # InflationæŒ‡æ ‡é…ç½® - 8ä¸ªæŒ‡æ ‡
        self.indicators = {
            'CPIAUCSL': {
                'name': 'Consumer Price Index (CPI)',
                'type': 'Inflation',
                'description': 'æ¶ˆè´¹è€…ä»·æ ¼æŒ‡æ•° - é€šèƒ€çš„é‡è¦è¡¡é‡æŒ‡æ ‡',
                'unit': 'Index 1982-1984=100',
                'validation': {
                    'min_value': 0,
                    'max_value': 1000,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'price_index'
                }
            },
            'PCEPILFE': {
                'name': 'Core PCE Price Index',
                'type': 'Inflation',
                'description': 'æ ¸å¿ƒä¸ªäººæ¶ˆè´¹æ”¯å‡ºä»·æ ¼æŒ‡æ•° - ç¾è”å‚¨å…³æ³¨çš„é€šèƒ€æŒ‡æ ‡',
                'unit': 'Index 2017=100',
                'validation': {
                    'min_value': 0,
                    'max_value': 500,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'price_index'
                }
            },
            'FEDFUNDS': {
                'name': 'Federal Funds Rate',
                'type': 'Inflation',
                'description': 'è”é‚¦åŸºé‡‘åˆ©ç‡ - å½±å“é€šèƒ€çš„å…³é”®è´§å¸æ”¿ç­–å·¥å…·',
                'unit': 'Percent',
                'validation': {
                    'min_value': 0,
                    'max_value': 25,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'interest_rate'
                }
            },
            'UNRATE': {
                'name': 'Unemployment Rate',
                'type': 'Inflation',
                'description': 'å¤±ä¸šç‡ - ä¸é€šèƒ€æœ‰å¯†åˆ‡å…³ç³»çš„å°±ä¸šæŒ‡æ ‡',
                'unit': 'Percent',
                'validation': {
                    'min_value': 0,
                    'max_value': 100,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'unemployment'
                }
            },
            'RSAFS': {
                'name': 'Retail Sales',
                'type': 'Inflation',
                'description': 'é›¶å”®é”€å”® - åæ˜ æ¶ˆè´¹éœ€æ±‚å’Œé€šèƒ€å‹åŠ›',
                'unit': 'Millions of Dollars',
                'validation': {
                    'min_value': 0,
                    'max_value': 2000000,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'retail_sales'
                }
            },
            'PPIACO': {
                'name': 'Producer Price Index: All Commodities',
                'type': 'Inflation',
                'description': 'ç”Ÿäº§è€…ä»·æ ¼æŒ‡æ•° - ä¸Šæ¸¸é€šèƒ€å‹åŠ›æŒ‡æ ‡',
                'unit': 'Index 1982=100',
                'validation': {
                    'min_value': 0,
                    'max_value': 1000,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'price_index'
                }
            },
            'T10YIEM': {
                'name': '10-Year Breakeven Inflation Rate',
                'type': 'Inflation',
                'description': '10å¹´ç›ˆäºå¹³è¡¡é€šèƒ€ç‡ - å¸‚åœºé€šèƒ€é¢„æœŸ',
                'unit': 'Percent',
                'validation': {
                    'min_value': -5,
                    'max_value': 15,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'inflation_expectation'
                }
            },
            'DCOILWTICO': {
                'name': 'Crude Oil Prices (WTI)',
                'type': 'Inflation',
                'description': 'åŸæ²¹ä»·æ ¼ - å½±å“é€šèƒ€çš„é‡è¦å•†å“ä»·æ ¼',
                'unit': 'Dollars per Barrel',
                'validation': {
                    'min_value': 0,
                    'max_value': 300,
                    'required_fields': ['date', 'value'],
                    'business_rules': 'oil_price'
                }
            }
        }
        
        # æ€§èƒ½ç›‘æ§
        self.start_time = time.time()
        self.processed_count = 0
        self.success_count = 0
        self.error_count = 0
        
        logger.info(f"âœ… æŠ“å–å™¨åˆå§‹åŒ–å®Œæˆï¼Œé…ç½®äº†{len(self.indicators)}ä¸ªInflationæŒ‡æ ‡")
    
    def validate_observation(self, series_id: str, date: str, value: str) -> Tuple[bool, Optional[float], str]:
        """éªŒè¯è§‚æµ‹æ•°æ®çš„æœ‰æ•ˆæ€§"""
        try:
            # è·å–éªŒè¯é…ç½®
            config = self.indicators.get(series_id, {}).get('validation', {})
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if not date or not value:
                return False, None, "ç¼ºå°‘å¿…éœ€å­—æ®µ"
            
            # æ•°å€¼è½¬æ¢éªŒè¯
            try:
                numeric_value = float(value)
            except (ValueError, TypeError):
                if value in ['.', '', 'null', 'None']:
                    return False, None, "ç©ºå€¼"
                return False, None, f"æ— æ•ˆæ•°å€¼: {value}"
            
            # èŒƒå›´éªŒè¯
            min_val = config.get('min_value', float('-inf'))
            max_val = config.get('max_value', float('inf'))
            
            if not (min_val <= numeric_value <= max_val):
                return False, None, f"æ•°å€¼è¶…å‡ºèŒƒå›´[{min_val}, {max_val}]: {numeric_value}"
            
            # ä¸šåŠ¡è§„åˆ™éªŒè¯
            business_rule = config.get('business_rules')
            if business_rule:
                rule_valid, rule_msg = self.validate_business_rule(series_id, numeric_value, business_rule)
                if not rule_valid:
                    return False, None, f"ä¸šåŠ¡è§„åˆ™éªŒè¯å¤±è´¥: {rule_msg}"
            
            return True, numeric_value, "éªŒè¯é€šè¿‡"
            
        except Exception as e:
            return False, None, f"éªŒè¯å¼‚å¸¸: {str(e)}"
    
    def validate_business_rule(self, series_id: str, value: float, rule_type: str) -> Tuple[bool, str]:
        """æ‰§è¡Œä¸šåŠ¡è§„åˆ™éªŒè¯"""
        try:
            if rule_type == 'price_index':
                # ä»·æ ¼æŒ‡æ•°ä¸åº”è¯¥ä¸ºè´Ÿæ•°ï¼Œä¸”å¢é•¿åº”è¯¥åˆç†
                if value < 0:
                    return False, f"ä»·æ ¼æŒ‡æ•°ä¸èƒ½ä¸ºè´Ÿ: {value}"
                if value > 1000:  # å¼‚å¸¸é«˜çš„ä»·æ ¼æŒ‡æ•°
                    return False, f"ä»·æ ¼æŒ‡æ•°å¼‚å¸¸è¿‡é«˜: {value}"
                return True, "ä»·æ ¼æŒ‡æ•°åˆç†"
                
            elif rule_type == 'interest_rate':
                # åˆ©ç‡éªŒè¯
                if value < 0:
                    return False, f"æ£€æµ‹åˆ°è´Ÿåˆ©ç‡: {value}%"
                if value > 25:
                    return False, f"åˆ©ç‡å¼‚å¸¸è¿‡é«˜: {value}%"
                return True, "åˆ©ç‡èŒƒå›´åˆç†"
                
            elif rule_type == 'unemployment':
                # å¤±ä¸šç‡éªŒè¯
                if value < 0 or value > 100:
                    return False, f"å¤±ä¸šç‡èŒƒå›´å¼‚å¸¸: {value}%"
                if value > 30:
                    return False, f"å¤±ä¸šç‡å¼‚å¸¸è¿‡é«˜: {value}%"
                return True, "å¤±ä¸šç‡æ­£å¸¸"
                
            elif rule_type == 'retail_sales':
                # é›¶å”®é”€å”®éªŒè¯
                if value < 0:
                    return False, f"é›¶å”®é”€å”®ä¸èƒ½ä¸ºè´Ÿ: {value}"
                return True, "é›¶å”®é”€å”®åˆç†"
                
            elif rule_type == 'inflation_expectation':
                # é€šèƒ€é¢„æœŸéªŒè¯
                if value < -5 or value > 15:
                    return False, f"é€šèƒ€é¢„æœŸèŒƒå›´å¼‚å¸¸: {value}%"
                return True, "é€šèƒ€é¢„æœŸåˆç†"
                
            elif rule_type == 'oil_price':
                # åŸæ²¹ä»·æ ¼éªŒè¯
                if value < 0:
                    return False, f"åŸæ²¹ä»·æ ¼ä¸èƒ½ä¸ºè´Ÿ: {value}"
                if value > 300:
                    return False, f"åŸæ²¹ä»·æ ¼å¼‚å¸¸è¿‡é«˜: {value}"
                return True, "åŸæ²¹ä»·æ ¼åˆç†"
            
            return True, "æ— ç‰¹å®šè§„åˆ™"
            
        except Exception as e:
            return False, f"ä¸šåŠ¡è§„åˆ™éªŒè¯å¼‚å¸¸: {str(e)}"
    
    def save_observations_batch(self, series_id: str, observations: List[Dict]) -> Tuple[int, int]:
        """æ‰¹é‡ä¿å­˜è§‚æµ‹æ•°æ®åˆ°æ•°æ®åº“"""
        saved_count = 0
        skipped_count = 0
        
        try:
            with transaction.atomic():
                for obs in observations:
                    try:
                        # éªŒè¯æ•°æ®
                        is_valid, numeric_value, validation_msg = self.validate_observation(
                            series_id, obs.get('date'), obs.get('value')
                        )
                        
                        if not is_valid:
                            skipped_count += 1
                            continue
                        
                        # ä½¿ç”¨get_or_createé¿å…é‡å¤
                        obj, created = FredUsIndicator.objects.get_or_create(
                            series_id=series_id,
                            date=obs['date'],
                            defaults={
                                'value': numeric_value
                            }
                        )
                        
                        if created:
                            saved_count += 1
                        else:
                            skipped_count += 1
                            
                    except IntegrityError:
                        skipped_count += 1
                        continue
                    except Exception as e:
                        logger.warning(f"ä¿å­˜è§‚æµ‹æ•°æ®å¼‚å¸¸ {series_id} {obs.get('date')}: {str(e)}")
                        skipped_count += 1
                        continue
                        
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜äº‹åŠ¡å¤±è´¥ {series_id}: {str(e)}")
            raise
        
        return saved_count, skipped_count
    
    def fetch_series_with_retry(self, series_id: str, limit: int = 1000, max_retries: int = 3) -> Optional[Dict]:
        """å¸¦é‡è¯•æœºåˆ¶çš„ç³»åˆ—æ•°æ®è·å–"""
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ”„ è·å– {series_id} æ•°æ® (å°è¯• {attempt + 1}/{max_retries})")
                
                # ä½¿ç”¨UsFredDataFetcherçš„æ–¹æ³•
                observations = self.fetcher.get_series_observations(series_id, limit=limit)
                
                if observations and len(observations) > 0:
                    logger.info(f"âœ… æˆåŠŸè·å– {series_id}: {len(observations)} æ¡è§‚æµ‹æ•°æ®")
                    return {
                        'series_id': series_id,
                        'observations': observations,
                        'count': len(observations)
                    }
                else:
                    logger.warning(f"âš ï¸  {series_id} æœªè¿”å›æ•°æ®")
                    
            except Exception as e:
                logger.error(f"âŒ è·å– {series_id} å¤±è´¥ (å°è¯• {attempt + 1}): {str(e)}")
                if attempt < max_retries - 1:
                    sleep_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                    logger.info(f"â³ ç­‰å¾… {sleep_time} ç§’åé‡è¯•...")
                    time.sleep(sleep_time)
                else:
                    logger.error(f"ğŸ’¥ {series_id} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡")
        
        return None
    
    def fetch_indicator(self, series_id: str) -> Dict:
        """è·å–å•ä¸ªæŒ‡æ ‡æ•°æ®"""
        logger.info(f"ğŸ“Š å¼€å§‹å¤„ç†æŒ‡æ ‡: {series_id}")
        start_time = time.time()
        
        result = {
            'series_id': series_id,
            'success': False,
            'total_fetched': 0,
            'total_saved': 0,
            'total_skipped': 0,
            'processing_time': 0,
            'error': None
        }
        
        try:
            # è·å–æŒ‡æ ‡é…ç½®
            indicator_config = self.indicators.get(series_id)
            if not indicator_config:
                raise ValueError(f"æœªæ‰¾åˆ°æŒ‡æ ‡é…ç½®: {series_id}")
            
            # è·å–æ•°æ®
            data = self.fetch_series_with_retry(series_id, limit=1000)
            if not data:
                raise Exception(f"æ— æ³•è·å– {series_id} çš„æ•°æ®")
            
            result['total_fetched'] = data['count']
            
            # æ‰¹é‡ä¿å­˜æ•°æ®
            observations = data['observations']
            batch_size = 100  # æ¯æ‰¹å¤„ç†100æ¡
            
            total_saved = 0
            total_skipped = 0
            
            for i in range(0, len(observations), batch_size):
                batch = observations[i:i + batch_size]
                saved, skipped = self.save_observations_batch(series_id, batch)
                total_saved += saved
                total_skipped += skipped
                
                logger.info(f"ğŸ“ˆ {series_id} æ‰¹æ¬¡ {i//batch_size + 1}: ä¿å­˜ {saved}, è·³è¿‡ {skipped}")
            
            result.update({
                'success': True,
                'total_saved': total_saved,
                'total_skipped': total_skipped,
                'processing_time': time.time() - start_time
            })
            
            # æ›´æ–°å…¨å±€ç»Ÿè®¡
            self.success_count += 1
            self.processed_count += result['total_fetched']
            
            logger.info(f"âœ… {series_id} å¤„ç†å®Œæˆ: è·å– {result['total_fetched']}, ä¿å­˜ {total_saved}, è·³è¿‡ {total_skipped}")
            
        except Exception as e:
            error_msg = str(e)
            result['error'] = error_msg
            self.error_count += 1
            logger.error(f"âŒ {series_id} å¤„ç†å¤±è´¥: {error_msg}")
        
        result['processing_time'] = time.time() - start_time
        return result
    
    def generate_summary_report(self, results: List[Dict]) -> Dict:
        """ç”Ÿæˆæ‰§è¡Œæ€»ç»“æŠ¥å‘Š"""
        total_time = time.time() - self.start_time
        
        total_fetched = sum(r.get('total_fetched', 0) for r in results)
        total_saved = sum(r.get('total_saved', 0) for r in results)
        total_skipped = sum(r.get('total_skipped', 0) for r in results)
        
        successful_indicators = [r for r in results if r.get('success')]
        failed_indicators = [r for r in results if not r.get('success')]
        
        report = {
            'execution_summary': {
                'total_indicators': len(self.indicators),
                'successful_indicators': len(successful_indicators),
                'failed_indicators': len(failed_indicators),
                'success_rate': (len(successful_indicators) / len(self.indicators)) * 100,
                'total_execution_time': total_time,
                'average_time_per_indicator': total_time / len(self.indicators)
            },
            'data_statistics': {
                'total_records_fetched': total_fetched,
                'total_records_saved': total_saved,
                'total_records_skipped': total_skipped,
                'save_success_rate': (total_saved / total_fetched) * 100 if total_fetched > 0 else 0,
                'processing_throughput': total_fetched / total_time if total_time > 0 else 0
            },
            'indicator_details': {
                'successful': [
                    {
                        'series_id': r['series_id'],
                        'name': self.indicators[r['series_id']]['name'],
                        'fetched': r['total_fetched'],
                        'saved': r['total_saved'],
                        'processing_time': round(r['processing_time'], 2)
                    }
                    for r in successful_indicators
                ],
                'failed': [
                    {
                        'series_id': r['series_id'],
                        'name': self.indicators.get(r['series_id'], {}).get('name', 'Unknown'),
                        'error': r.get('error', 'Unknown error')
                    }
                    for r in failed_indicators
                ]
            }
        }
        
        return report
    
    def run_full_fetch(self) -> Dict:
        """æ‰§è¡Œå®Œæ•´çš„Inflationæ•°æ®æŠ“å–æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹InflationæŒ‡æ ‡æ•°æ®æŠ“å–æµç¨‹")
        logger.info("="*80)
        
        results = []
        
        # é€ä¸ªå¤„ç†æŒ‡æ ‡
        for series_id, config in self.indicators.items():
            logger.info(f"ğŸ¯ å¤„ç†æŒ‡æ ‡ {series_id}: {config['name']}")
            result = self.fetch_indicator(series_id)
            results.append(result)
            
            # çŸ­æš‚æš‚åœé¿å…APIé™åˆ¶
            time.sleep(1)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        logger.info("="*80)
        logger.info("ğŸ“Š ç”Ÿæˆæ‰§è¡Œæ€»ç»“æŠ¥å‘Š")
        report = self.generate_summary_report(results)
        
        # è¾“å‡ºæŠ¥å‘Š
        self.print_summary_report(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = f'/Volumes/Pickle Samsung SSD/MEM Dashboard 2/logs/inflation_fetch_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}")
        
        return report
    
    def print_summary_report(self, report: Dict):
        """æ‰“å°æ ¼å¼åŒ–çš„æ€»ç»“æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š INFLATION æ•°æ®æŠ“å–æ‰§è¡Œæ€»ç»“")
        print("="*80)
        
        exec_summary = report['execution_summary']
        data_stats = report['data_statistics']
        
        print(f"ğŸ¯ æŒ‡æ ‡å¤„ç†æ¦‚è§ˆ:")
        print(f"   æ€»æŒ‡æ ‡æ•°é‡: {exec_summary['total_indicators']}")
        print(f"   æˆåŠŸæŒ‡æ ‡: {exec_summary['successful_indicators']}")
        print(f"   å¤±è´¥æŒ‡æ ‡: {exec_summary['failed_indicators']}")
        print(f"   æˆåŠŸç‡: {exec_summary['success_rate']:.1f}%")
        
        print(f"\nâ±ï¸  æ€§èƒ½æŒ‡æ ‡:")
        print(f"   æ€»æ‰§è¡Œæ—¶é—´: {exec_summary['total_execution_time']:.2f} ç§’")
        print(f"   å¹³å‡å¤„ç†æ—¶é—´: {exec_summary['average_time_per_indicator']:.2f} ç§’/æŒ‡æ ‡")
        print(f"   å¤„ç†ååé‡: {data_stats['processing_throughput']:.1f} æ¡/ç§’")
        
        print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        print(f"   è·å–è®°å½•æ€»æ•°: {data_stats['total_records_fetched']:,}")
        print(f"   ä¿å­˜è®°å½•æ€»æ•°: {data_stats['total_records_saved']:,}")
        print(f"   è·³è¿‡è®°å½•æ€»æ•°: {data_stats['total_records_skipped']:,}")
        print(f"   ä¿å­˜æˆåŠŸç‡: {data_stats['save_success_rate']:.1f}%")
        
        # æˆåŠŸæŒ‡æ ‡è¯¦æƒ…
        if report['indicator_details']['successful']:
            print(f"\nâœ… æˆåŠŸå¤„ç†çš„æŒ‡æ ‡:")
            for indicator in report['indicator_details']['successful']:
                print(f"   â€¢ {indicator['series_id']}: {indicator['name']}")
                print(f"     è·å–: {indicator['fetched']}, ä¿å­˜: {indicator['saved']}, è€—æ—¶: {indicator['processing_time']}s")
        
        # å¤±è´¥æŒ‡æ ‡è¯¦æƒ…
        if report['indicator_details']['failed']:
            print(f"\nâŒ å¤±è´¥çš„æŒ‡æ ‡:")
            for indicator in report['indicator_details']['failed']:
                print(f"   â€¢ {indicator['series_id']}: {indicator['name']}")
                print(f"     é”™è¯¯: {indicator['error']}")
        
        print("\n" + "="*80)

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    try:
        logger.info("ğŸš€ å¯åŠ¨InflationæŒ‡æ ‡æ•°æ®æŠ“å–å™¨")
        
        # åˆ›å»ºå¹¶æ‰§è¡ŒæŠ“å–å™¨
        fetcher = InflationDataFetcher()
        report = fetcher.run_full_fetch()
        
        # æ£€æŸ¥æ‰§è¡Œç»“æœ
        if report['execution_summary']['success_rate'] >= 80:
            logger.info("ğŸ‰ InflationæŒ‡æ ‡æ•°æ®æŠ“å–æˆåŠŸå®Œæˆ")
            return 0
        else:
            logger.warning("âš ï¸  éƒ¨åˆ†æŒ‡æ ‡æŠ“å–å¤±è´¥ï¼Œä½†æ•´ä½“å®Œæˆ")
            return 1
            
    except KeyboardInterrupt:
        logger.warning("âš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        return 130
    except Exception as e:
        logger.error(f"ğŸ’¥ æŠ“å–è¿‡ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
        return 2

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
